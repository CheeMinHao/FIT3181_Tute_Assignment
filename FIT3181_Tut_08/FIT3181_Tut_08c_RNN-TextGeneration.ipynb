{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2022)</span>\n",
    "***\n",
    "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
    "*Tutor:*  **Mr Thanh Nguyen** \\[Thanh.Nguyen4@monash.edu \\] |**Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] |**Mr Anh Bui** \\[tuananh.bui@monash.edu\\] | **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu \\] | **Mr Md Mohaimenuzzaman** \\[md.mohaimen@monash.edu \\] |**Mr James Tong** \\[james.tong1@monash.edu \\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***on Technology, Monash University, Australia\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 08c (Additional Reading): RNN for Text Generation</span> <span style=\"color:red\">***</span> #\n",
    "\n",
    "This tutorial is designed to show one of the applications of RNN in generating texts or sequences. Basically, we train an RNN using the maximum log-likelihood principle and then use this trained RNN to generate texts that imitate the existed texts in the dataset we trained our RNN on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the necessary modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">I. Download and preprocess data</span> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \".\"\n",
    "CHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.mkdir(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function helps to download the dataset at a specific URL and split the sentences into characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(urls):\n",
    "    texts = []\n",
    "    for i, url in enumerate(urls):\n",
    "        p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url, cache_dir=\".\")\n",
    "        text = open(p, \"r\", encoding=\"utf8\").read()\n",
    "        # remove byte order mark\n",
    "        text = text.replace(\"\\ufeff\", \"\")\n",
    "        # remove newlines\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(r'\\s+', \" \", text)\n",
    "        # add it to the list\n",
    "        texts.extend(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the dataset and the variable *texts* is a list containing all characters of the sentences in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = download_and_read([\"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\", \"https://www.gutenberg.org/files/12/12-0.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'G', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', \"'\", 's', ' ', 'A', 'l', 'i', 'c', 'e', \"'\", 's', ' ', 'A', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'W', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n', 'd', ',', ' ', 'b', 'y', ' ', 'L', 'e', 'w', 'i', 's', ' ', 'C', 'a', 'r', 'r', 'o', 'l', 'l', ' ', 'T', 'h', 'i', 's', ' ', 'e', 'B', 'o', 'o', 'k', ' ', 'i', 's', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'u', 's', 'e', ' ', 'o', 'f', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the vocabulary of all unique characters in this dataset and store in *vocab*. In addition, we have two dictionaries: *char2idx* and *idx2char* to convert between the characters and their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 90\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary\n",
    "vocab = sorted(set(texts))\n",
    "print(\"vocab size: {:d}\".format(len(vocab)))\n",
    "# create mapping from vocab chars to ints\n",
    "char2idx = {c:i for i, c in enumerate(vocab)}\n",
    "idx2char = {i:c for c, i in char2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the characters in *texts* to the indices in *texts_as_ints* and then make a Tensorflow dataset *data* from this *texts_as_ints*. Finally, we chop *data* into batch dataset *sequences*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericize the texts\n",
    "texts_as_ints = np.array([char2idx[c] for c in texts])\n",
    "data = tf.data.Dataset.from_tensor_slices(texts_as_ints)\n",
    "# number of characters to show before asking for prediction\n",
    "# sequences: [None, 100]\n",
    "seq_length = 100\n",
    "sequences = data.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the below function, you can imagine *sequence* is a batch of characters, for example \\['I', 'l', 'o', 'v', 'e', 'D', 'L'\\], this function will return \\['I', 'l', 'o', 'v', 'e', 'D'\\] and \\['l', 'o', 'v', 'e', 'D', 'L'\\].\n",
    "\n",
    "The idea later is that we feed \\['I', 'l', 'o', 'v', 'e', 'D'\\] to our RNN and try to predict \\['l', 'o', 'v', 'e', 'D', 'L'\\] which is the set of next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_labels(sequence):\n",
    "    input_seq = sequence[0:-1]\n",
    "    output_seq = sequence[1:]\n",
    "    return input_seq, output_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the function *split_train_labels* to each batch in sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = sequences.map(split_train_labels)\n",
    "# set up for training\n",
    "# batches: [None, 64, 100]\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(texts) // seq_length // batch_size\n",
    "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encapsulate our generation model in the class *CharGenModel*. Our model has one embedding layer and one hidden layer with GRU cells. Note that we need to set *return_sequences=True* for the hidden layer so that it returns a 3D tensor of all hidden values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharGenModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, **kwargs):\n",
    "        super(CharGenModel, self).__init__(**kwargs)\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn_layer = tf.keras.layers.GRU(embedding_dim, recurrent_initializer=\"glorot_uniform\", recurrent_activation=\"sigmoid\", \n",
    "                                             stateful=True, return_sequences=True)\n",
    "        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.rnn_layer(x)\n",
    "        x = self.dense_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_output_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharGenModel(vocab_size, embedding_dim)\n",
    "model.build(input_shape=(batch_size, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss function which is the sum of the loss at each time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, predictions):\n",
    "    return tf.losses.sparse_categorical_crossentropy(labels,predictions,from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a text, we start from a prefix_string. We convert this string to a list of indices and declare a 2D tensor from this list with the first dimension to be $1$. We feed *inputs* to the model to work out the prediction probability *preds* and sample *pred_id* from this probability and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prefix_string, char2idx, idx2char, num_chars_to_generate=1000, temperature=1.0):\n",
    "    inputs = [char2idx[s] for s in prefix_string]\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    for i in range(num_chars_to_generate):\n",
    "        preds = model(inputs)\n",
    "        preds = tf.squeeze(preds, 0) / temperature\n",
    "        # predict char returned by model\n",
    "        pred_id = tf.random.categorical(preds, num_samples=1)[-1, 0].numpy()\n",
    "        text_generated.append(idx2char[pred_id])\n",
    "        # pass the prediction as the next input to the model\n",
    "        inputs = tf.expand_dims([pred_id], 0)\n",
    "    return prefix_string + \"\".join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 [==============================] - 12s 200ms/step - loss: 3.2570\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 11s 210ms/step - loss: 2.5288\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 11s 206ms/step - loss: 2.3123\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 11s 209ms/step - loss: 2.1570\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 12s 214ms/step - loss: 2.0280\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 11s 208ms/step - loss: 1.9278\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 11s 208ms/step - loss: 1.8339\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 11s 210ms/step - loss: 1.7597\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 11s 206ms/step - loss: 1.6934\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 11s 206ms/step - loss: 1.6399\n",
      "Alice Adies turned with no agoors, in this for ell full be igick/dly much, sto looks in round agstly dooch the Firsty, you for his thooking thought of the lastle being the other veris, and undo to etenct but replect of the Pigrantely, ‘Wown sud into camen thinct Gutence,’ ‘All she its the is,\" said ithe beant the DO*” she thind dreat again ofxing of pleas head--they: ‘You’d tay mp anx be manate, Twith pactive. I. He said at herontwards of it mane gettered it way dearts!’ ‘Then she Cat again so the was that, theme time sour the subleered got elon. \"Yellave the could!’ ‘she had pootembthe upon’t for_ of watchistly!\" ‘I his ligute to rush says betien at the which just dwourdany _I't_ poout the Gutenberg-ty justed n. ‘Revera'k! ‘And the crill. I'm YOU, and she causly. \"I dear was my,’ she time anly thempity exco into to smake, Let* ‘Cere won by the said to for ase Se, \"You’rt stidned downer, brang, she wilence. ‘Onlyter about the much harre would,’ ‘Yery say best, chill, chea--lo-1 and have Alic\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 12s 209ms/step - loss: 1.5905\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 11s 209ms/step - loss: 1.5471\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 11s 209ms/step - loss: 1.5114\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 11s 210ms/step - loss: 1.4798\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 12s 215ms/step - loss: 1.4483\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 11s 210ms/step - loss: 1.4222\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 12s 215ms/step - loss: 1.3971\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 12s 216ms/step - loss: 1.3791\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 12s 216ms/step - loss: 1.3599\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 11s 210ms/step - loss: 1.3389\n",
      "Alice precestably little gentures yet, to cam--in a little tolfucuptants he to court easy strangh she tame of the other, called at all a little thaw _herea_.' ‘A satclee hands in theer, and said to her, she oge. \"Aw I glad new d suddened to all exacted dewyread here,’ she went the bitters, who are. ‘They're presenty?’ said the Rabbit, with the kepped forms: the wable, and with. If it like in the right quite acrowder to tremualf grow----\" \"Who was goingrily. \"So she couldn’t want here? The King, ‘they starl so pointered that Doubtant, shut out (I tone. Hull Englight of thrsem tear mouse faugh, of course _I_ cristen sogeth: Only a creaticulty! But she lovided for do.\" \"Yee), In't YOU [Sidethout accain donding voice for statifel, and were curious Faster, you knum--for swo, mockison. ‘I’m go dons: if a refund tumbering of little stimar VERY, Project Gutenberg-tm electronic would near a some, you know all the her!’ The Mock Turh another moment. Could shate paw, Snickaving rook. She say, when I ce\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 12s 217ms/step - loss: 1.3233\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 12s 227ms/step - loss: 1.3081\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 12s 220ms/step - loss: 1.2917\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 11s 211ms/step - loss: 1.2766\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 12s 219ms/step - loss: 1.2660\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 12s 215ms/step - loss: 1.2490\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 12s 222ms/step - loss: 1.2408\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 12s 227ms/step - loss: 1.2255\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 12s 219ms/step - loss: 1.2138\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 12s 216ms/step - loss: 1.2032\n",
      "Alice this, lest to her, so els, lives seemed round, to make out for in. I Alice had help it--‘Oh, do you like about, to long propications.’ ‘Dight and they looked and su deep do.’ ‘I beg a pig, 'to like came upon pretemn went appear! Such as a sovericatelf. ‘Do roves in the great prowitional take wou’de the Queen of you?\" \"I’m someght, he smught it countrocuit the table to don. \"But the bott bard--come? My, I know.’ The Lion and on. \"Rine by the wood day.\" \"Come!\" said the March and shuch the cook, ‘I know.’ ‘I say not many down, and sair I should never matterly.) ‘Brighty said? Th SUR?’ said the Dodm would have all over this time, and Alice had been 'talking. The Looking in his head--and and the Queen went on as she kinds come, pleated was a livel as somerial the dream of the Frog more comed on which with great about in the distory And on when you turned close to be a Canough.\" \"What another done than eise me the gave to sun! And then thought they must rememark even “no road--’ The King. \"\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 12s 209ms/step - loss: 1.1918\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 12s 218ms/step - loss: 1.1828\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 11s 202ms/step - loss: 1.1710\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 11s 204ms/step - loss: 1.1639\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 12s 217ms/step - loss: 1.1514\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 12s 231ms/step - loss: 1.1373\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 12s 217ms/step - loss: 1.1310\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 12s 217ms/step - loss: 1.1249\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 12s 217ms/step - loss: 1.1158\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 11s 212ms/step - loss: 1.1054\n",
      "Alice is eal preased them) of the corrects of suddenly, \"for circled all the officers will have you a good wain in a work climing half enough;\" Alice knowing: ‘and! ‘There’s no nearly clashe replacement with one almoothat shook (T wand.) \"Nost--but it's--Hold, Oy put on white king.\" ‘The judiely dowl; and the morasim of this agreement ALICT copying,\" said Alice, looking about?’ (And not like _you_ pull you must be Latears, you know what you’re there was beginnimposith here?\" And her very remarked hastily. ‘You have most provisionich her one of insect it what the fud this work with any shoffie. Now mure to pivined their had anywhere join piecing-tm and-hand startled rofuld by three while she ventured to picked off high; through another intempting her two argument-feasing the officital people \"You gallow?\" the Duchess. \"Whink yet it had _will_ make any poor Alice looked stood frosting her in a seemed to Telie; Looking of prectused: she was?’ Alice will brions?\" The Red Queen. ‘Thank you just w\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 12s 210ms/step - loss: 1.0976\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 11s 208ms/step - loss: 1.0888\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 11s 204ms/step - loss: 1.0800\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 11s 208ms/step - loss: 1.0722\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 11s 207ms/step - loss: 1.0638\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 11s 209ms/step - loss: 1.0571\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 11s 210ms/step - loss: 1.0465\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 11s 210ms/step - loss: 1.0383\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 11s 210ms/step - loss: 1.0355\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 11s 211ms/step - loss: 1.0256\n",
      "Alice (which was to have no idea with Hims. If you silent with her immediate at the hill you: any of tears--all,’ said the King. ‘How I mean) that it was that it was no copy. Alice went on, ‘if I saw old, Micsion. Mood to ter end by, thre word about her head--‘that's volunteers about herself the pictures folded in would, which shaightent. ‘Oh, that you can do it again, which was sit would get off. ‘I say.\" \"'_Well, that I Bus, toke the other, about doubthrain would go an over the soldier \"We seemed to this, but chiny to you to Alice was so long,’ Alice said, growing with get of breadifully as she heard it set first he lasse exembetion of Project Gutenberg-tm----\" \"We, and the moraty is E at was again: \"you kngen humbly. ‘Or tea. \"Perhint (a bit, and can be used to) Parts, she had to leave off these, thatchide and strong!\" the March Hare and the slare of Paursity. \"I never was Alice looked upon her birds,’ she asked, without needled nothing what then Alice did not squee last. ‘You lig a garde\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "num_epochs = 50\n",
    "for i in range(num_epochs // 10):\n",
    "    model.fit(dataset.repeat(), epochs=10, steps_per_epoch=steps_per_epoch)\n",
    "    checkpoint_file = os.path.join(CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n",
    "    model.save_weights(checkpoint_file)\n",
    "    gen_model = CharGenModel(vocab_size, embedding_dim)\n",
    "    gen_model.load_weights(checkpoint_file)\n",
    "    gen_model.build(input_shape=(1, None))\n",
    "    # create generative model using the trained model so far\n",
    "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
