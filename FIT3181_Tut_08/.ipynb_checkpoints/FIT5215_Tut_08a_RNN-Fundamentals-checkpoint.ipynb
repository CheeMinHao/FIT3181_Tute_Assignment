{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT5215: Deep Learning (Summer Semester A 2021)</span>\n",
    "***\n",
    "*CE/Lecturer:*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head TA:*  **Mr Tuan Nguyen**  \\[tuan.nguyen2@monash.edu \\] <br/>\n",
    "*Tutor:* **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu\\] | **Mr Thanh Nguyen** \\[thanh.nguyen4@monash.edu\\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 07a: Fundamental of RNN in TF 2.x</span> <span style=\"color:red\">*****</span> #\n",
    "\n",
    "This tutorial is designed to facilitate you in understanding the fundamental building blocks of a Recurrent Neural Network (RNN) including:\n",
    "- The computational process of a standard and simple RNN cell.\n",
    "- How to declare and manipulate with standard RNN, LSTM, and GRU cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">I. Fundamental of RNN</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.1. Manual RNN</span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement a simple basic RNN on our own. This basic RNN has two hidden states to take two inputs (i.e., the sequence length is $2$).\n",
    "\n",
    "The computation process is as follows:\n",
    "- $h_0 = tanh(X_0 \\times U + b)$.\n",
    "- $h_1 = tanh(X_1 \\times U + h_0 \\times W +b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, $X0$ is a mini-batch with batch size $4$ consisting of  the data of time step $0$.\n",
    "- $X0$'s shape is $[batch\\_size \\times input\\_size]$\n",
    "\n",
    "$X1$ is a mini-batch with batch size $4$ consisting of  the data of time step $1$.\n",
    "- $X1$'s shape is $[batch\\_size \\times input\\_size]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X0 = np.array([[0.0, 1.0, -2.0], \n",
    "               [-3.0, 4.0, 5.0], \n",
    "               [6.0, 7.0, -8.0],\n",
    "               [6.0, -1.0, 2.0]], dtype= np.float32) # t = 0\n",
    "X1 = np.array([[9.0, 8.0, 7.0], \n",
    "               [0.0, 0.0, 0.0], \n",
    "               [6.0, 5.0, 4.0],\n",
    "               [1.0, 2.0, 3.0]], dtype= np.float32) # t = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now demonstrate the computational process for a standard RNN with sequence length $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 5\n",
    "input_size = 3\n",
    "\n",
    "U = tf.Variable(tf.random.normal(shape=[input_size, hidden_size],dtype=tf.float32))\n",
    "W = tf.Variable(tf.random.normal(shape=[hidden_size, hidden_size],dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, hidden_size], dtype=tf.float32))\n",
    "\n",
    "h0 = tf.tanh(tf.matmul(X0, U) + b)\n",
    "h1 = tf.tanh(tf.matmul(X1, U) + tf.matmul(h0, W)  + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h0= [[ 0.98877776  0.40096563 -0.9882086   0.2189422   0.2372293 ]\n",
      " [ 0.99893343 -0.3624211  -0.99938715 -0.9997827  -0.99907583]\n",
      " [ 0.99999577  0.89227045 -0.9999949  -1.          1.        ]\n",
      " [-1.         -0.71483064  1.         -0.99999934  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"h0= {}\".format(h0.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1= [[-1.         -0.8157639   0.99999374 -1.          1.        ]\n",
      " [ 0.44075513 -0.98258084  0.35103258  0.6746963   0.47123832]\n",
      " [-0.9999708  -0.6903518   0.99974215 -1.          1.        ]\n",
      " [-0.7057077  -0.6433726   0.9994588  -1.          0.98326075]]\n"
     ]
    }
   ],
   "source": [
    "print(\"h1= {}\".format(h1.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 1</span>:** Explain why $h_0$  and  $h_1$  have the above shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 2</span>:** Extend to a given $L$ time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.2. Recurrent cells in Tensorflow Keras</span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Keras supports most of the necessary recurrent cells (layers) which you might need in your real projects. The following figure shows all recurrent cells (layers) supported by TF Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/RNN_layers.png\" align=\"left\" width=180/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Simple RNN cell</span> ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the introduction of the *standard and simple RNN cell*. The following figure shows the signature of the class *tf.keras.layers.SimpleRNN* and its parameters.\n",
    "\n",
    "<img src=\"./images/SimpleRNN_cell.png\" align=\"left\" width=1200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks a bit complicated. However, there are some important things you need to be clear about now. \n",
    "\n",
    "First, you can imagine that your RNN consists of many recurrent layers, each of which consists of many cells (e.g., simple RNN cell, LSTM cell, and GRU cell). You need to input to a recurrent layer a 3D tensor with the shape $batch\\_size \\times timesteps \\times input\\_size$ and take the output as a 3D tensor with the shape $batch\\_size \\times timesteps \\times output\\_size$.\n",
    "- $batch\\_size$ means the number of sequences (sentences) in a mini-batch, $timesteps$ means the sequence length or number of tokens/words in your sequences, and $input\\_size$ specifies the input size of each token. Later you will know that for symbolic tokens like words, we need to embed them to feature vectors using an embedding matrix.\n",
    "\n",
    "Moreover, by default, the *output* returned by a recurrent cell/layer is the last hidden value (i.e., the value of the last cell or $h_L$).\n",
    "\n",
    "Second, the meaning of the parameters *return_state* and *return_sequences*.\n",
    "- *return_state = True* indicates that the last hidden state (i.e., $h_L$) will be returned in addition to the output.\n",
    "- *return_sequences = True* indicates that the concatenation of all hidden values for of all hidden cells ($[h_1, h_2,...,h_L]$) will be returned in addition to the output in the form of a 3D tensor with the shape $batch\\_size \\times timesteps \\times hidden\\_size$. Otherwise, it returns the last hidden state $h_L$ with the shape $batch\\_size \\times hidden\\_size$,\n",
    "\n",
    "Note that regarding the terminologies, there are some equivalent terms that you need to pay attention to:\n",
    "- `timesteps = seq_length` or sequence length which specifies the number of cells in a recurrent layer.\n",
    "- `state_size = hidden_size` which represents the common size of cells in a given recurrent layer, meaning the common size of $h_1, h_2,...,h_L$ where $L$ is the sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/all_in_once.png\" align=\"left\" width=1200/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture above shows the illustration of RNN architecture. Basically, we have three recurrent layers (e.g., *Hidden 1, Hidden 2, and Hidden 3*). Each recurrent layer consists of many GRU cells.\n",
    "- Input to Hidden 1 layer is 3D tensor $batch\\_size \\times seq\\_len \\times embed\\_size$ and output is the 3D tensor $batch\\_size \\times seq\\_len \\times state\\_size_1$. Here $state\\_size_1$ is the common hidden state size of all GRU cells on the Hidden 1 layer.\n",
    "- Input to Hidden 2 layer is 3D tensor $batch\\_size \\times seq\\_len \\times state\\_size_1$ and output is the 3D tensor $batch\\_size \\times seq\\_len \\times state\\_size_2$. Here $state\\_size_2$ is the common hidden state size of all GRU cells on the Hidden 2 layer.\n",
    "- Input to Hidden 3 layer is 3D tensor $batch\\_size \\times seq\\_len \\times state\\_size_2$ and output is the 3D tensor $batch\\_size \\times seq\\_len \\times state\\_size_3$. Here $state\\_size_3$ is the common hidden state size of all GRU cells on the Hidden 3 layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding how to transform a batch of sentences to sequences of indices as a 2D tensor $batch\\_size \\times seq\\_len$ and then use the embedding layer to further transform to a 3D tensor with the shape $batch\\_size \\times seq\\_len \\times embed\\_size$, please refer to Tute 8b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code only returns $output$ as the last hidden value with the shape $32 \\times 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.random([32, 10, 8]).astype(np.float32)\n",
    "simple_rnn = tf.keras.layers.SimpleRNN(4)\n",
    "\n",
    "output = simple_rnn(inputs)  # The output has shape `[32, 4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set *return_sequences=True* to return  $whole\\_sequence\\_output$  including all hidden values of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn = tf.keras.layers.SimpleRNN(4, return_sequences=True, return_state=True)\n",
    "\n",
    "# whole_sequence_output has shape `[32, 10, 4]`.\n",
    "# final_state has shape `[32, 4]`.\n",
    "whole_sequence_output, final_state = simple_rnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 4)\n",
      "(32, 4)\n"
     ]
    }
   ],
   "source": [
    "print(whole_sequence_output.shape)\n",
    "print(final_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 3</span>:** Please run the following code and explain why we obtain the corresponding result (pay attention to $return\\_sequences=False, return\\_state=True$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn = tf.keras.layers.SimpleRNN(4, return_sequences=False, return_state=True)\n",
    "\n",
    "# whole_sequence_output has shape `[32, 10, 4]`.\n",
    "# final_state has shape `[32, 4]`.\n",
    "output, final_state = simple_rnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the same\n"
     ]
    }
   ],
   "source": [
    "bool_arr = final_state.numpy()==output.numpy()\n",
    "bool_list = bool_arr.ravel().tolist()\n",
    "and_all = all(bool_list)\n",
    "print(\"the same\" if and_all else \"different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">LSTM cell</span> ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM cell/layer is encapsulated in *the class tf.keras.layers.LSTM*. The signature and parameters of the LSTM class are similar to that of the standard RNN cell class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.random.normal([32, 10, 8])\n",
    "lstm = tf.keras.layers.LSTM(4)\n",
    "output = lstm(inputs)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference between the LSTM cell and other cells: when setting $return\\_sequences=True$, it will return the final long-term memory $c_L$ as shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 4)\n",
      "(32, 4)\n",
      "(32, 4)\n"
     ]
    }
   ],
   "source": [
    "lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
    "whole_seq_output, final_memory_state, final_hidden_state = lstm(inputs)\n",
    "print(whole_seq_output.shape)    #h = [h1, h2,..., hL]\n",
    "print(final_memory_state.shape)  #cL\n",
    "print(final_hidden_state.shape)   #hL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">GRU cell</span> ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU cell/layer is packaged in *the class tf.keras.layers.GRU*. The signature and paramters of the GRU class are similar to that of the standard RNN cell class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.random.normal([32, 10, 8])\n",
    "gru = tf.keras.layers.GRU(4)\n",
    "output = gru(inputs)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 4)\n",
      "(32, 4)\n"
     ]
    }
   ],
   "source": [
    "gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)\n",
    "whole_sequence_output, final_hidden_state = gru(inputs)\n",
    "print(whole_sequence_output.shape)\n",
    "print(final_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
