{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2021)</span>\n",
    "***\n",
    "*CE/Lecturer:*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head TA:*  **Mr Thanh Nguyen** | thanh.nguyen4@monash.edu <br/>\n",
    "*Tutor:* **Dr Van Nguyen**  \\[van.nguyen1@monash.edu \\] | **Mr James Tong** \\[james.tong1@monash.edu\\] | **Dr Mahmoud Mohammad** \\[mahmoud.hossam@monash.edu\\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 09b: RNNs with Word2Vec</span> <span style=\"color:red\">*****</span> #\n",
    "\n",
    "This tutorial will show you how to use a pretrained Word2Vec to initialize the embedding matrix of RNNs used for a given task for example sentence classification or sentiment analysis. Instead of randomly initializing the embedding matrix, when initializing that matrix using a pretrained Word2Vec, we take advantage of the linguistic/semantic relationships the pretrained Word2Vec drawn from the large text corpus it was trained on (e.g., 100 billion words from a Google News dataset and contains a vocabulary of 3 million words and phrases). \n",
    "\n",
    "More specifically, we build up an RNN for *spam SMS detection* for which the embedding matrix is initialized from a pretrained Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import some necessary packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">I. Introduction of the SMS spam detection dataset</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset which we investigate in this tutorial lab is the SMS spam detection dataset. The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged according to being ham (legitimate) or spam. More information on this dataset can be found [here](https://www.kaggle.com/uciml/sms-spam-collection-dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">II. Load and preprocess the dataset</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the class *DataManager* as a hub that helps us to load, preprocess, manipulate, and build up the necessary vocabulary and dictionaries (word2idx or idx2word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, url= None):\n",
    "        self.url = url\n",
    "        self.max_seq_len = None       # store the max sequence length\n",
    "        self.num_sentences = None     # store number of sentences \n",
    "        self.texts = None             # store all sentences\n",
    "        self.labels = None            # store all labels\n",
    "        self.nums_seqs = None         # store sequences of indices \n",
    "        self.vocab_size = None\n",
    "        \n",
    "    \n",
    "    def read_data(self, file_path):\n",
    "        df = pd.read_csv(file_path, encoding = \"ISO-8859-1\")\n",
    "        labels, texts = df['v1'], df['v2']\n",
    "        self.texts= texts\n",
    "        self.labels = labels    \n",
    "    \n",
    "    def transform_to_numbers(self):\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(self.texts)\n",
    "        self.nums_seqs = self.tokenizer.texts_to_sequences(self.texts)\n",
    "        self.nums_seqs = tf.keras.preprocessing.sequence.pad_sequences(self.nums_seqs, padding='post')\n",
    "        le = LabelEncoder()\n",
    "        le.fit(self.labels)\n",
    "        self.nums_labels = le.transform(self.labels) \n",
    "        self.max_seq_len = len(self.nums_seqs[0])\n",
    "        self.num_sentences = len(self.nums_seqs)\n",
    "        \n",
    "    def build_vocabulary(self):\n",
    "        self.word2idx = self.tokenizer.word_index\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.min_index = min(self.word2idx.values())\n",
    "        self.max_index = max(self.word2idx.values())\n",
    "        \n",
    "    def process_data(self):\n",
    "        self.transform_to_numbers()\n",
    "        self.build_vocabulary()\n",
    "        \n",
    "        \n",
    "    def train_valid_test_split(self, train_ratio= 0.8, test_ratio=0.1):\n",
    "        valid_ratio = 1 - (train_ratio + test_ratio)\n",
    "        train_size = int(self.num_sentences*train_ratio) +1\n",
    "        test_size = int(self.num_sentences*test_ratio) +1\n",
    "        valid_size = self.num_sentences - (train_size + test_size)\n",
    "        data_set = tf.data.Dataset.from_tensor_slices((self.nums_seqs, self.nums_labels))\n",
    "        data_set = data_set.shuffle(1000)\n",
    "        self.train_set = data_set.take(train_size)\n",
    "        self.valid_set = data_set.skip(train_size)\n",
    "        self.test_set = data_set.skip(train_size + valid_size)\n",
    "        \n",
    "    def print_infor(self, num_samples = 5):\n",
    "        print(\"Here are some statistics and examples from the dataset\")\n",
    "        if self.num_sentences is not None:\n",
    "            print(\"+ Dataset has {} sentences\".format(self.num_sentences))\n",
    "        if self.vocab_size is not None:\n",
    "            print(\"+ Vocabulary size is {} with min index= {}, max index= {}\".format(self.vocab_size, self.min_index, self.max_index))\n",
    "        if self.max_seq_len is not None:\n",
    "            print(\"+ The max sequence length is {}\".format(self.max_seq_len))\n",
    "        if self.texts is not None:\n",
    "            print(\"\\nHere are some text samples\")\n",
    "            for i in range(num_samples):\n",
    "                print(\"+ Text: {}\\n+ Indices: {}\\n+ Label: {} ({})\\n\".format(self.texts[i], self.nums_seqs[i],self.labels[i], self.nums_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.read_data(\"./datasets/spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some statistics and examples from the dataset\n",
      "+ Dataset has 5572 sentences\n",
      "+ Vocabulary size is 8920 with min index= 1, max index= 8920\n",
      "+ The max sequence length is 189\n",
      "\n",
      "Here are some text samples\n",
      "+ Text: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "+ Indices: [  50  469 4410  841  751  657   64    8 1324   89  121  349 1325  147\n",
      " 2987 1326   67   58 4411  144    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "+ Label: ham (0)\n",
      "\n",
      "+ Text: Ok lar... Joking wif u oni...\n",
      "+ Indices: [  46  336 1495  470    6 1929    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "+ Label: ham (0)\n",
      "\n",
      "+ Text: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "+ Indices: [  47  486    8   19    4  796  899    2  178 1930 1199  658 1931 2320\n",
      "  267 2321   71 1930    2 1932    2  337  486  554  955   73  388  179\n",
      "  659  389 2988    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "+ Label: spam (1)\n",
      "\n",
      "+ Text: U dun say so early hor... U c already then say...\n",
      "+ Indices: [   6  245  152   23  379 2989    6  140  154   57  152    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "+ Label: ham (0)\n",
      "\n",
      "+ Text: Nah I don't think he goes to usf, he lives around here though\n",
      "+ Indices: [1018    1   98  107   69  487    2  956   69 1933  218  111  471    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "+ Label: ham (0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dm.print_infor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">III. Build the RNN model</span> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class *RNN_Spam_Detection* represents the RNN for SMS spam detection. There are some important attributes (properties or instance variables) of this class:\n",
    "- `run_mode=scratch or init-fine-tune` specifies the fact we train embedding matrix from scratch or initialize its weights using the pretrained Word2Vect model and then do fine-tuning.\n",
    "- `embed_model` indicates the pretrained Word2Vect model we use to initialize the embedding matrix. Note that in this case, the embedding size is specified by the number at the end (e.g., glove-wiki-gigaword-300).\n",
    "- `embed_size` specifies the embedding size and is also the hidden size of the first hidden layer of memory cells. Note that if the running mode is not *scratch*, we set the embedding size as specified by the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Spam_Detection:\n",
    "    def __init__(self, run_mode=\"scratch\", embed_model=\"glove-wiki-gigaword-300\", embed_size=128, data_manager=None):\n",
    "        self.embed_path = \"embeddings/E.npy\"\n",
    "        self.embed_model = embed_model\n",
    "        self.embed_size = embed_size\n",
    "        if run_mode != 'scratch':\n",
    "            self.embed_size = int(self.embed_model.split(\"-\")[-1])\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1  \n",
    "        self.word2idx = self.data_manager.word2idx\n",
    "        self.embed_matrix = np.zeros((self.vocab_size, self.embed_size))\n",
    "        self.run_mode = run_mode\n",
    "        self.model = None\n",
    "    \n",
    "    def build_embedding_matrix(self):\n",
    "        if os.path.exists(self.embed_path): # file existed\n",
    "            self.embed_matrix = np.load(self.embed_path) # Load the file for embedding matrix if existed\n",
    "        else: # file not existed or first-time run\n",
    "            self.word2vect = api.load(self.embed_model) # load embedding model\n",
    "            for word, idx in self.word2idx.items():\n",
    "                try:\n",
    "                    self.embed_matrix[idx] = self.word2vect.word_vec(word) # assign weight for the corresponding word and index\n",
    "                except KeyError: # word cannot be found\n",
    "                    pass\n",
    "            np.save(self.embed_path, self.embed_matrix)\n",
    "    \n",
    "    def build(self):\n",
    "        inputs = tf.keras.layers.Input(shape=[None])\n",
    "        if self.run_mode == \"scratch\":\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=True)\n",
    "        else: # fine-tuned\n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=True,\n",
    "                                                        weights=[self.embed_matrix])\n",
    "        h = self.embedding_layer(inputs)\n",
    "        h = tf.keras.layers.GRU(256, return_sequences=True)(h)\n",
    "        h = tf.keras.layers.GRU(128)(h)\n",
    "        h = tf.keras.layers.Dense(1, activation=\"sigmoid\")(h)\n",
    "        self.model = tf.keras.Model(inputs= inputs, outputs=h)\n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">III.1. Run in the running mode of training from scratch</span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set random seeds for both numpy and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(6789)\n",
    "np.random.seed(6789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1 = RNN_Spam_Detection(data_manager=dm, run_mode=\"scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1.compile_model(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.train_valid_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70/70 [==============================] - 42s 596ms/step - loss: 0.5273 - accuracy: 0.8607 - val_loss: 0.4353 - val_accuracy: 0.8698\n",
      "Epoch 2/5\n",
      "70/70 [==============================] - 49s 704ms/step - loss: 0.4181 - accuracy: 0.8663 - val_loss: 0.3895 - val_accuracy: 0.8770\n",
      "Epoch 3/5\n",
      "70/70 [==============================] - 51s 733ms/step - loss: 0.4048 - accuracy: 0.8659 - val_loss: 0.3884 - val_accuracy: 0.8743\n",
      "Epoch 4/5\n",
      "70/70 [==============================] - 53s 760ms/step - loss: 0.4010 - accuracy: 0.8668 - val_loss: 0.3708 - val_accuracy: 0.8833\n",
      "Epoch 5/5\n",
      "70/70 [==============================] - 52s 738ms/step - loss: 0.4048 - accuracy: 0.8643 - val_loss: 0.4040 - val_accuracy: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e784d386d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn1.model.fit(dm.train_set.batch(64), epochs=5, validation_data= dm.valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 146ms/step - loss: 0.3849 - accuracy: 0.8746\n"
     ]
    }
   ],
   "source": [
    "rnn1.evaluate(dm.test_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">III.2. Run in the running mode of fine-tuning the embedding matrix</span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2 = RNN_Spam_Detection(data_manager=dm, run_mode=\"init-fine-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2.compile_model(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70/70 [==============================] - 69s 984ms/step - loss: 0.4063 - accuracy: 0.8428 - val_loss: 0.3172 - val_accuracy: 0.8761\n",
      "Epoch 2/5\n",
      "70/70 [==============================] - 75s 1s/step - loss: 0.3252 - accuracy: 0.8650 - val_loss: 0.3193 - val_accuracy: 0.8636\n",
      "Epoch 3/5\n",
      "70/70 [==============================] - 76s 1s/step - loss: 0.3042 - accuracy: 0.8656 - val_loss: 0.2919 - val_accuracy: 0.8725\n",
      "Epoch 4/5\n",
      "70/70 [==============================] - 79s 1s/step - loss: 0.2896 - accuracy: 0.8659 - val_loss: 0.2852 - val_accuracy: 0.8707\n",
      "Epoch 5/5\n",
      "70/70 [==============================] - 79s 1s/step - loss: 0.2696 - accuracy: 0.8764 - val_loss: 0.2540 - val_accuracy: 0.8878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e797f19e20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2.model.fit(dm.train_set.batch(64), epochs=5, validation_data= dm.valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 2s 240ms/step - loss: 0.2519 - accuracy: 0.9032\n"
     ]
    }
   ],
   "source": [
    "rnn2.evaluate(dm.test_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
