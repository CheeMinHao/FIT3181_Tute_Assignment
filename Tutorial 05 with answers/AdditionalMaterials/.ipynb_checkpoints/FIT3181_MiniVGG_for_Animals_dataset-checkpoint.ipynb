{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2021)</span>\n",
    "***\n",
    "*CE/Lecturer:*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head TA:*  **Mr Thanh Nguyen** | thanh.nguyen4@monash.edu <br/>\n",
    "*Tutor:* **Dr Van Nguyen**  \\[van.nguyen1@monash.edu \\] | **Mr James Tong** \\[james.tong1@monash.edu\\] | **Dr Mahmoud Mohammad** \\[mahmoud.hossam@monash.edu\\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with MiniVGG for Animals Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "import imutils\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePreprocessor:\n",
    "    def __init__(self, width, height, inter= cv2.INTER_AREA):\n",
    "        self.width= width\n",
    "        self.height = height\n",
    "        self.inter = inter\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        image= cv2.resize(image, (self.width, self.height), interpolation = self.inter)\n",
    "        b,g,r= cv2.split(image)\n",
    "        image= cv2.merge((r,g,b))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalsDatasetManager:\n",
    "    def __init__(self, preprocessors=None):\n",
    "        self.cur_pos=0\n",
    "        self.preprocessors = preprocessors\n",
    "        if self.preprocessors is None:\n",
    "            self.preprocessors = list()\n",
    "    \n",
    "    def load(self, label_folder_dict, max_num_images=500, verbose =-1):\n",
    "        data =list(); labels = list()\n",
    "        for label, folder in label_folder_dict.items():\n",
    "            image_paths = list(paths.list_images(folder))\n",
    "            print(label, len(image_paths))\n",
    "            for (i, image_path) in enumerate(image_paths):\n",
    "                image = cv2.imread(image_path)\n",
    "                if self.preprocessors is not None:\n",
    "                    for p in self.preprocessors:\n",
    "                        image = p.preprocess(image)\n",
    "                data.append(image); labels.append(label)\n",
    "                if verbose > 0 and i>0 and (i+1)% verbose ==0:\n",
    "                    print(\"Processed {}/{}\".format(i+1, max_num_images))\n",
    "                if i+1 >= max_num_images:\n",
    "                    break\n",
    "        self.data= np.array(data)\n",
    "        self.labels= np.array(labels)\n",
    "        self.train_size= int(self.data.shape[0])\n",
    "    \n",
    "    def process_data_label(self):\n",
    "        label_encoder= preprocessing.LabelEncoder()\n",
    "        label_encoder.fit(self.labels)\n",
    "        self.labels= label_encoder.transform(self.labels)\n",
    "        self.data= self.data.astype(\"float\")/255.0\n",
    "        self.classes= label_encoder.classes_\n",
    "    \n",
    "    def train_valid_test_split(self, train_size=0.8, test_size= 0.1, rand_seed=33):\n",
    "        valid_size = 1 - (train_size + test_size)\n",
    "        X1, X_test, y1, y_test = train_test_split(self.data, self.labels, test_size = test_size, random_state= rand_seed)\n",
    "        self.X_test= X_test\n",
    "        self.y_test= y_test\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X1, y1, test_size = float(valid_size)/(valid_size+ train_size))\n",
    "        self.X_train= X_train\n",
    "        self.y_train= y_train\n",
    "        self.X_valid= X_valid\n",
    "        self.y_valid= y_valid\n",
    "    \n",
    "    def next_batch(self, batch_size=32):\n",
    "        end_pos= self.cur_pos + batch_size\n",
    "        x_batch= []\n",
    "        y_batch= []\n",
    "        if end_pos <= self.train_size:\n",
    "            x_batch= self.X_train[self.cur_pos:end_pos, :]\n",
    "            y_batch= self.y_train[self.cur_pos:end_pos]\n",
    "            self.cur_pos= end_pos\n",
    "        else:\n",
    "            cur_pos_new= (end_pos-1) % self.train_size +1\n",
    "            x_batch= np.concatenate((self.X_train[self.cur_pos: self.train_size, :], self.X_train[0:cur_pos_new,:]))\n",
    "            y_batch= np.concatenate((self.y_train[self.cur_pos: self.train_size], self.y_train[0:cur_pos_new]))\n",
    "            self.cur_pos= cur_pos_new\n",
    "        return x_batch, y_batch\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_folder_dict(adir):\n",
    "    sub_folders= [folder for folder in os.listdir(adir)\n",
    "                  if os.path.isdir(os.path.join(adir, folder))]\n",
    "    label_folder_dict= dict()\n",
    "    for folder in sub_folders:\n",
    "        item= {folder: os.path.abspath(os.path.join(adir, folder))}\n",
    "        label_folder_dict.update(item)\n",
    "    return label_folder_dict\n",
    "\n",
    "label_folder_dict= create_label_folder_dict(\"./Data/Animals\")\n",
    "sp = SimplePreprocessor(width=32, height=32)\n",
    "data_manager = AnimalsDatasetManager([sp])\n",
    "data_manager.load(label_folder_dict, verbose=100)\n",
    "data_manager.process_data_label()\n",
    "data_manager.train_valid_test_split()\n",
    "print(data_manager.X_train.shape, data_manager.y_train.shape)\n",
    "print(data_manager.X_valid.shape, data_manager.y_valid.shape)\n",
    "print(data_manager.X_test.shape, data_manager.y_test.shape)\n",
    "print(data_manager.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers:\n",
    "    @staticmethod\n",
    "    def dense(inputs, output_size, name=\"dense1\", act=None):\n",
    "        with tf.name_scope(name):\n",
    "            input_size= int(inputs.get_shape()[1])\n",
    "            W_init = tf.random_normal([input_size, output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "            b_init= tf.random_normal([output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "            W= tf.Variable(W_init, name= \"W\")\n",
    "            b= tf.Variable(b_init, name=\"b\")\n",
    "            Wxb= tf.matmul(inputs, W) + b\n",
    "            if act is None:\n",
    "                return Wxb\n",
    "            else:\n",
    "                return act(Wxb)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv2D(inputs, filter_shape, strides=[1,1,1,1], padding=\"SAME\", name= \"conv1\", act=None):\n",
    "        with tf.name_scope(name):\n",
    "            W_init= tf.random_normal(filter_shape, mean=0, stddev=0.1, dtype= tf.float32)\n",
    "            W= tf.Variable(W_init, name=\"W\")\n",
    "            b_init= tf.random_normal([int(filter_shape[3])], mean=0, stddev=0.1, dtype= tf.float32)\n",
    "            b= tf.Variable(b_init, name=\"b\")\n",
    "            Wxb= tf.nn.conv2d(input= inputs, filter= W, strides= strides, padding= padding)+b\n",
    "            if act is None:\n",
    "                return Wxb\n",
    "            else:\n",
    "                return act(Wxb)\n",
    "    @staticmethod\n",
    "    def max_pool(inputs, ksize=[1,2,2,1],strides=[1,2,2,1], padding=\"SAME\"):\n",
    "        return tf.nn.max_pool(value= inputs, ksize=ksize, strides= strides, padding= padding)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout(inputs, keep_prob):\n",
    "        return tf.nn.dropout(inputs, keep_prob= keep_prob)\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_norm(inputs, phase_train):\n",
    "        return tf.contrib.layers.batch_norm(inputs, decay= 0.99, \n",
    "                                            is_training=phase_train, center= True, scale=True, reuse= False)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVGGNet():\n",
    "    def __init__(self, width=32, height=32, depth=3, num_classes=4, keep_prob= 0.8,\n",
    "                 batch_size=10, epochs= 20, optimizer= tf.train.AdamOptimizer(learning_rate=0.0001), learning_rate=0.0001):\n",
    "        tf.reset_default_graph()\n",
    "        self.width= width\n",
    "        self.height= height\n",
    "        self.depth= depth\n",
    "        self.num_classes= num_classes\n",
    "        self.keep_prob= keep_prob\n",
    "        self.batch_size= batch_size\n",
    "        self.epochs= epochs\n",
    "        self.optimizer= optimizer\n",
    "        self.optimizer.learning_rate= learning_rate\n",
    "        self.session= tf.Session()\n",
    "        \n",
    "    def build(self):\n",
    "        self.X= tf.placeholder(shape=[None, self.height, self.width, self.depth], dtype=tf.float32)\n",
    "        self.y= tf.placeholder(shape= [None], dtype= tf.int64)\n",
    "        self.keep_prob_holder= tf.placeholder(dtype= tf.float32)\n",
    "        self.phase_train= tf.placeholder(dtype= tf.bool)\n",
    "        conv1= Layers.conv2D(inputs=self.X, filter_shape=[3,3,3,32], act= tf.nn.relu)\n",
    "        #bn1= Layers.batch_norm(conv1, self.phase_train)\n",
    "        conv2= Layers.conv2D(conv1, [3,3,32,32], act= tf.nn.relu)\n",
    "        #bn2= Layers.batch_norm(conv2, self.phase_train)\n",
    "        pool2= Layers.max_pool(conv2)\n",
    "        pool2_drop= Layers.dropout(pool2, self.keep_prob_holder)\n",
    "        conv3= Layers.conv2D(pool2_drop, [3,3,32,64], act= tf.nn.relu)\n",
    "        #bn3= Layers.batch_norm(conv3, self.phase_train)\n",
    "        conv4= Layers.conv2D(conv3, [3,3,64,64], act= tf.nn.relu)\n",
    "        #bn4= Layers.batch_norm(conv4, self.phase_train)\n",
    "        pool4= Layers.max_pool(conv4)\n",
    "        pool4_drop= Layers.dropout(pool4, self.keep_prob_holder)\n",
    "        pool4_flat= tf.reshape(pool4_drop,[-1, 8*8*64])\n",
    "        full5= Layers.dense(pool4_flat, 512, act= tf.nn.relu)\n",
    "        #bn5= Layers.batch_norm(full5, self.phase_train)\n",
    "        full5_drop= Layers.dropout(pool4_flat, self.keep_prob_holder)\n",
    "        self.outputs= Layers.dense(full5_drop, self.num_classes)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            cross_entropy= tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits= self.outputs)\n",
    "            self.loss= tf.reduce_mean(cross_entropy)\n",
    "            self.train= self.optimizer.minimize(self.loss)\n",
    "        with tf.name_scope(\"predict\"):\n",
    "            self.y_pred= tf.argmax(self.outputs, 1)\n",
    "            corrections= tf.equal(self.y_pred, self.y)\n",
    "            self.accuracy= tf.reduce_mean(tf.cast(corrections, tf.float32))\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def partial_fit(self, X_batch, y_batch):\n",
    "        self.session.run([self.train], feed_dict={self.X:X_batch, self.y:y_batch, \n",
    "                                                  self.keep_prob_holder: self.keep_prob, self.phase_train: True})\n",
    "        \n",
    "    def predict(self, X, y):\n",
    "        y_pred, acc= self.session.run([self.y_pred, self.accuracy], feed_dict={self.X:X, self.y:y, self.keep_prob_holder:1,\n",
    "                                                                              self.phase_train: False})\n",
    "        return y_pred, acc\n",
    "        \n",
    "    def compute_acc_loss(self, X, y):\n",
    "        loss, acc = self.session.run([self.loss, self.accuracy], \n",
    "                                             feed_dict={self.X:X, self.y:y, self.keep_prob_holder:1, self.phase_train: False})\n",
    "        return loss, acc\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.session.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "epochs=100\n",
    "num_classes= len(data_manager.classes)\n",
    "training_size= data_manager.X_train.shape[0]\n",
    "iter_per_epoch= int(training_size/batch_size) +1\n",
    "network= MiniVGGNet(batch_size= batch_size, epochs= epochs, num_classes= num_classes)\n",
    "network.build()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(iter_per_epoch):\n",
    "        X_batch,y_batch= data_manager.next_batch(batch_size)\n",
    "        network.partial_fit(X_batch, y_batch)\n",
    "    train_loss, train_acc= network.compute_acc_loss(data_manager.X_train, data_manager.y_train)\n",
    "    val_loss, val_acc= network.compute_acc_loss(data_manager.X_valid, data_manager.y_valid)\n",
    "    print(\"Epoch {}: train loss={}, val loss={}\\n\".format(epoch, train_loss, val_loss))\n",
    "    print(\"########: train acc={}, val acc={}\\n\".format(train_acc, val_acc))\n",
    "    #print(train_out, val_out)\n",
    "print(\"Finish training and come to testing\")\n",
    "y_test, acc= network.predict(data_manager.X_test, data_manager.y_test)\n",
    "print(\"Testing accuracy= {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
