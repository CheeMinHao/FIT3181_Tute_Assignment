{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2021)</span>\n",
    "***\n",
    "*CE/Lecturer:*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head TA:*  **Mr Thanh Nguyen** | thanh.nguyen4@monash.edu <br/>\n",
    "*Tutor:* **Dr Van Nguyen**  \\[van.nguyen1@monash.edu \\] | **Mr James Tong** \\[james.tong1@monash.edu\\] | **Dr Mahmoud Mohammad** \\[mahmoud.hossam@monash.edu\\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Classification with Convolutional Neural Network</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf \n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose= verbose\n",
    "        self.max_sentence_len= 0\n",
    "        self.questions= list()\n",
    "        self.str_labels= list()\n",
    "        self.numeral_labels= list()\n",
    "        self.numeral_data= list()\n",
    "        self.cur_pos=0\n",
    "        \n",
    "    def maybe_download(self, dir_name, file_name, url):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if self.verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "    \n",
    "    def read_data(self, dir_name, file_name):\n",
    "        file_path= os.path.join(dir_name, file_name)\n",
    "        self.questions= list(); self.labels= list()\n",
    "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "            for row in f:\n",
    "                row_str= row.split(\":\")\n",
    "                label, question= row_str[0], row_str[1]\n",
    "                question= question.lower()\n",
    "                self.labels.append(label)\n",
    "                self.questions.append(question.split())\n",
    "                if self.max_sentence_len < len(self.questions[-1]):\n",
    "                    self.max_sentence_len= len(self.questions[-1])\n",
    "        le= preprocessing.LabelEncoder()\n",
    "        le.fit(self.labels)\n",
    "        self.numeral_labels = le.transform(self.labels)\n",
    "        self.str_classes= le.classes_\n",
    "        self.num_classes= len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"Sample questions \\n\")\n",
    "            print(self.questions[0:5])\n",
    "            print(\"Labels {}\\n\\n\".format(self.str_classes))\n",
    "    \n",
    "    def padding(self, length):\n",
    "        for question in self.questions:\n",
    "            question= question.extend([\"pad\"]*(length- len(question)))\n",
    "    \n",
    "    def build_numeral_data(self, dictionary):\n",
    "        self.numeral_data= list()\n",
    "        for question in self.questions:\n",
    "            data= list()\n",
    "            for word in question:\n",
    "                data.append(dictionary[word])\n",
    "            self.numeral_data.append(data)\n",
    "        if self.verbose:\n",
    "            print('Sample numeral data \\n')   \n",
    "            print(self.numeral_data[0:5])\n",
    "    \n",
    "    def train_valid_split(self, train_size=0.9, rand_seed=33):\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(np.array(self.numeral_data), np.array(self.numeral_labels), \n",
    "                                                            test_size = 1-train_size, random_state= rand_seed)\n",
    "        self.train_numeral= X_train\n",
    "        self.train_labels= y_train\n",
    "        self.valid_numeral= X_valid\n",
    "        self.valid_labels= y_valid\n",
    "        \n",
    "    @staticmethod\n",
    "    def build_dictionary_count(questions):\n",
    "        count= []\n",
    "        dictionary= dict()\n",
    "        words= []\n",
    "        for question in questions:\n",
    "            words.extend(question)\n",
    "        count.extend(collections.Counter(words).most_common())\n",
    "        for word,freq in count:\n",
    "            dictionary[word]= len(dictionary)\n",
    "        reverse_dictionary= dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        return dictionary, reverse_dictionary, count\n",
    "    \n",
    "    def next_batch(self, batch_size, vocab_size, input_len):\n",
    "        data_batch= np.zeros([batch_size, input_len, vocab_size])\n",
    "        label_batch= np.zeros([batch_size, self.num_classes])\n",
    "        train_size= len(self.train_numeral)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(input_len):\n",
    "                data_batch[i,j, self.train_numeral[self.cur_pos][j]]=1\n",
    "            label_batch[i, self.train_labels[self.cur_pos]]=1\n",
    "            self.cur_pos= (self.cur_pos+1)%train_size\n",
    "        return data_batch, label_batch\n",
    "    \n",
    "    def convert_to_feed(self, data_numeral, label_numeral, input_len, vocab_size):\n",
    "        data2feed= np.zeros([data_numeral.shape[0], input_len, vocab_size])\n",
    "        label2feed= np.zeros([data_numeral.shape[0], self.num_classes])\n",
    "        for i in range(data_numeral.shape[0]):\n",
    "            for j in range(input_len):\n",
    "                data2feed[i,j, data_numeral[i][j]]=1\n",
    "            label2feed[i, label_numeral[i]]=1\n",
    "        return data2feed, label2feed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dm = DataManager()\n",
    "train_dm.maybe_download(\"Data/question-classif-data\", \"train_1000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "test_dm = DataManager()\n",
    "test_dm.maybe_download(\"Data/question-classif-data\", \"TREC_10.label\", \"http://cogcomp.org/Data/QA/QC/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dm.read_data(\"Data/question-classif-data\", \"train_1000.label\")\n",
    "test_dm.read_data(\"Data/question-classif-data\", \"TREC_10.label\")\n",
    "pad_len = max(train_dm.max_sentence_len, test_dm.max_sentence_len)\n",
    "train_dm.padding(pad_len)\n",
    "test_dm.padding(pad_len)\n",
    "all_questions= list(train_dm.questions) \n",
    "all_questions.extend(test_dm.questions)\n",
    "dictionary,_,_= DataManager.build_dictionary_count(all_questions)\n",
    "train_dm.build_numeral_data(dictionary)\n",
    "test_dm.build_numeral_data(dictionary)\n",
    "train_dm.train_valid_split()\n",
    "data_batch, label_batch= train_dm.next_batch(batch_size=5, vocab_size= len(dictionary), input_len= pad_len)\n",
    "print(\"Sample data batch- label batch \\n\")\n",
    "print(data_batch)\n",
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers:\n",
    "    @staticmethod\n",
    "    def dense(inputs, output_size, name=\"dense1\", act=None):\n",
    "        with tf.name_scope(name):\n",
    "            input_size= int(inputs.get_shape()[1])\n",
    "            W_init = tf.random_normal([input_size, output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "            b_init= tf.random_normal([output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "            W= tf.Variable(W_init, name= \"W\")\n",
    "            b= tf.Variable(b_init, name=\"b\")\n",
    "            Wxb= tf.matmul(inputs, W) + b\n",
    "            if act is None:\n",
    "                return Wxb\n",
    "            else:\n",
    "                return act(Wxb)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv2D(inputs, filter_shape, strides=[1,1,1,1], padding=\"SAME\", name= \"conv1\", act=None):\n",
    "        with tf.name_scope(name):\n",
    "            W_init= tf.random_normal(filter_shape, mean=0, stddev=0.1, dtype= tf.float32)\n",
    "            W= tf.Variable(W_init, name=\"W\")\n",
    "            b_init= tf.random_normal([int(filter_shape[3])], mean=0, stddev=0.1, dtype= tf.float32)\n",
    "            b= tf.Variable(b_init, name=\"b\")\n",
    "            Wxb= tf.nn.conv2d(input= inputs, filter= W, strides= strides, padding= padding)+b\n",
    "            if act is None:\n",
    "                return Wxb\n",
    "            else:\n",
    "                return act(Wxb)\n",
    "            \n",
    "    @staticmethod\n",
    "    def conv1D(inputs, filter_shape, stride=1, padding=\"SAME\", name=\"conv1\", act=None):\n",
    "        with tf.name_scope(name):\n",
    "            W_init= tf.random_normal(filter_shape, mean=0, stddev=0.1, dtype= tf.float32)\n",
    "            W= tf.Variable(W_init, name=\"W\")\n",
    "            b_init= tf.random_normal([filter_shape[2]], mean=0, stddev=0.1)\n",
    "            b= tf.Variable(b_init, name=\"b\")\n",
    "            Wxb= tf.nn.conv1d(value=inputs, filters=W, stride= stride, padding= padding) +b\n",
    "            if act is None:\n",
    "                return Wxb\n",
    "            else:\n",
    "                return act(Wxb)\n",
    "    \n",
    "    @staticmethod\n",
    "    def max_pool(inputs, ksize=[1,2,2,1],strides=[1,2,2,1], padding=\"SAME\"):\n",
    "        return tf.nn.max_pool(value= inputs, ksize=ksize, strides= strides, padding= padding)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout(inputs, keep_prob):\n",
    "        return tf.nn.dropout(inputs, keep_prob= keep_prob)\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_norm(inputs, phase_train):\n",
    "        return tf.contrib.layers.batch_norm(inputs, decay= 0.99, \n",
    "                                            is_training=phase_train, center= True, scale=True, reuse= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SC_CNN:\n",
    "    def __init__(self, height, width, batch_size=32, epochs=100, num_classes=5, save_history= True, \n",
    "                 verbose= True, optimizer= tf.train.AdamOptimizer(learning_rate=0.001), learning_rate=0.001):\n",
    "        tf.reset_default_graph()\n",
    "        self.height= height \n",
    "        self.width= width\n",
    "        self.batch_size= batch_size\n",
    "        self.epochs= epochs\n",
    "        self.num_classes= num_classes\n",
    "        self.optimizer= optimizer\n",
    "        self.optimizer.learning_rate= learning_rate\n",
    "        self.verbose= verbose\n",
    "        self.save_history= save_history\n",
    "        if self.save_history:\n",
    "            self.H= {\"train_loss_batch\": [], \"train_acc_batch\": [], \"train_loss_epoch\": [], \n",
    "                     \"train_acc_epoch\": [], \"valid_loss_epoch\": [], \"valid_acc_epoch\": []}\n",
    "        self.session= tf.Session()\n",
    "    \n",
    "    def build(self):\n",
    "        self.X= tf.placeholder(shape= [None, self.height, self.width], dtype=tf.float32)\n",
    "        self.y= tf.placeholder(shape= [None, self.num_classes], dtype= tf.float32)\n",
    "        conv1= Layers.conv1D(inputs= self.X, filter_shape= [3, self.width, 1], name=\"conv1\")\n",
    "        conv2= Layers.conv1D(inputs= self.X, filter_shape= [5, self.width, 1], name=\"conv2\")\n",
    "        conv3= Layers.conv1D(inputs= self.X, filter_shape= [7, self.width, 1], name=\"conv3\")\n",
    "        h1=tf.reduce_max(conv1, axis=1)\n",
    "        h2=tf.reduce_max(conv2, axis=1)\n",
    "        h3= tf.reduce_max(conv3, axis=1)\n",
    "        h= tf.concat([h1,h2,h3], axis=1)\n",
    "        logits= Layers.dense(inputs= h, output_size= self.num_classes)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            cross_entropy= tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits= logits)\n",
    "            self.loss= tf.reduce_mean(cross_entropy)\n",
    "            self.train= self.optimizer.minimize(self.loss)\n",
    "        with tf.name_scope(\"predict\"):\n",
    "            self.y_pred= tf.argmax(logits, axis=1)\n",
    "            y1= tf.argmax(self.y, axis=1)\n",
    "            corrections= tf.cast(tf.equal(self.y_pred, y1), dtype=tf.float32)\n",
    "            self.accuracy= tf.reduce_mean(corrections)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def partial_fit(self, data_batch, label_batch):\n",
    "        self.session.run([self.train], feed_dict={self.X:data_batch, self.y:label_batch})\n",
    "        if self.save_history:\n",
    "            self.compute_loss_acc(data_batch, label_batch, \"train\", \"Iteration\", 1)\n",
    "    \n",
    "    def predict(self, X,y):\n",
    "        y_pred, acc= self.session.run([self.y_pred, self.accuracy], feed_dict={self.X:X, self.y:y})\n",
    "        return y_pred, acc\n",
    "    \n",
    "    def compute_loss_acc(self, X, y, applied_set=\"train\", applied_scope=\"Epoch\", index= 1):\n",
    "        loss, acc= self.session.run([self.loss, self.accuracy], feed_dict={self.X:X, self.y:y})\n",
    "        if self.verbose and applied_scope==\"Epoch\":\n",
    "            print(\"{} {} {} loss= {}, acc={}\".format(applied_scope, index, applied_set, loss, acc))\n",
    "        if self.save_history:\n",
    "            if applied_scope==\"Iteration\":\n",
    "                self.H[\"train_loss_batch\"].append(loss)\n",
    "                self.H[\"train_acc_batch\"].append(acc)\n",
    "            else:\n",
    "                if applied_set==\"train\":\n",
    "                    self.H[\"train_loss_epoch\"].append(loss)\n",
    "                    self.H[\"train_acc_epoch\"].append(acc)\n",
    "                else:\n",
    "                    self.H[\"valid_loss_epoch\"].append(loss)\n",
    "                    self.H[\"valid_acc_epoch\"].append(acc)    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "epochs= 100\n",
    "train_size= len(train_dm.train_numeral)\n",
    "iter_per_epoch= math.ceil(train_size/batch_size)\n",
    "network= SC_CNN(height= pad_len, width= len(dictionary),batch_size=batch_size, epochs= epochs, num_classes= train_dm.num_classes)\n",
    "network.build()\n",
    "\n",
    "train2feed, train_label2feed= train_dm.convert_to_feed(train_dm.train_numeral, train_dm.train_labels, \n",
    "                                     input_len= pad_len, vocab_size=len(dictionary))\n",
    "\n",
    "valid2feed, valid_label2feed=  train_dm.convert_to_feed(train_dm.valid_numeral, train_dm.valid_labels, \n",
    "                                     input_len= pad_len, vocab_size=len(dictionary))\n",
    "\n",
    "test2feed, test_label2feed= test_dm.convert_to_feed(np.array(test_dm.numeral_data), np.array(test_dm.numeral_labels), \n",
    "                                     input_len= pad_len, vocab_size=len(dictionary))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(iter_per_epoch):\n",
    "        data_batch, label_batch= train_dm.next_batch(batch_size= batch_size, \n",
    "                                                      vocab_size=len(dictionary), input_len= pad_len)\n",
    "        #print(data_batch.shape, label_batch.shape)\n",
    "        network.partial_fit(data_batch, label_batch)\n",
    "    network.compute_loss_acc(train2feed, train_label2feed, \"train\", \"Epoch\", epoch +1)\n",
    "    network.compute_loss_acc(valid2feed, valid_label2feed, \"valid\", \"Epoch\", epoch +1)\n",
    "print(\"Finish training and computing testing performance\\n\")\n",
    "y_pred, test_acc= network.predict(test2feed, test_label2feed)\n",
    "print(\"Testing accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(history[\"train_loss_epoch\"], \"r^-\", label=\"train loss epoch\")\n",
    "    plt.plot(history[\"valid_loss_epoch\"], \"b*-\", label= \"valid loss epoch\")\n",
    "    plt.legend()\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(history[\"train_acc_epoch\"], \"r^-\", label=\"train acc epoch\")\n",
    "    plt.plot(history[\"valid_acc_epoch\"], \"b*-\", label= \"valid acc epoch\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history(network.H)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
